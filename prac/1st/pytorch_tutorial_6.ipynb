{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637653c7-1c08-4611-9048-fe4c4d5c39d2",
   "metadata": {},
   "source": [
    "# TORCH.AUYOGRAD를 사용한 자동 미분\n",
    "- 신경망을 학습할 때 가장 자주 사용되는 알고리즘은 역전파이다. 이 때 매개변수느 주어진 매개변수에 대한 송싱함수의 gradient로 조정된다.\n",
    "- pytorch에 torch.autograd()는 자동 미분 엔진이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc3cf4-453a-4583-8cac-4d62a82a82dd",
   "metadata": {},
   "source": [
    "### Tensor, fuction과 연산그래프(Computational graph)\n",
    "- 신경망에서 w와b는 최적화를 해야 하는 매개변수이다. 따라서 이러한 변수들에 대한 손실 함수의 변화도르 계산할 수 있어야한다.\n",
    "- 이를 위해서 해당 텐서에 requires_grad속성을 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96872d8a-4f6c-436f-aed4-38a7d68c3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8c9fc0-74aa-40e7-92a1-b504821f65f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x104aef490>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x104aef370>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c632ad-4426-4151-b5b1-b8193bb9ea80",
   "metadata": {},
   "source": [
    "### 변화도(Gradient) 계산하기\n",
    "- 신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대란 손실함수의 도함수(dericative)를 계산해야한다.\n",
    "- 이러한 도함수를 계산하기 위해 loss.backward()를 호출한 다음 w.grad와 b.grad에서 값을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "221225cd-6fc9-4438-97be-65ab70907288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2508, 0.2877, 0.2710],\n",
      "        [0.2508, 0.2877, 0.2710],\n",
      "        [0.2508, 0.2877, 0.2710],\n",
      "        [0.2508, 0.2877, 0.2710],\n",
      "        [0.2508, 0.2877, 0.2710]])\n",
      "tensor([0.2508, 0.2877, 0.2710])\n"
     ]
    }
   ],
   "source": [
    "# requires_grad가 True인 노드들의 grad만 구할 수 있다. backward호출이 여러 번 필요하면 retrain_graph=True를 사용\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6c388-0c33-4362-8626-d9c48315b1d5",
   "metadata": {},
   "source": [
    "### 변화도 추적 멈추기\n",
    "- requires_grad=True인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을 지원한다.\n",
    "- 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파연산만 필요한 경우헤는 torch.no_grad()블록으로 둘러싸서 연산 추적을 멈출 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5206260e-43f6-4985-9b2a-07457a60adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "# 동일한 결과 dstach()사용\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e98a1-3a94-44a0-b0ba-6ff5e7cb828e",
   "metadata": {},
   "source": [
    "변화도 투적을 멈춰야 하는 이유\n",
    "- 신경망의 일부 매개변수를 고정된 매개변수로 표기한다. 이는 사전 학습된 신경망을 미세조정 할 때 매우 일반적인 시나리오이다.\n",
    "- 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단게만 수행할 때 연산 속도가 향상된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59aa3a6-1ece-47cc-8562-bfb625cdcaf6",
   "metadata": {},
   "source": [
    "### 연산 그래프에 대한 추가 정보\n",
    "- autograd는 텐서의 실행된 모든 연산들의 기록을 Function객체로 구성된 비순화 그래프(DAG)에 저장한다.\n",
    "- 이 DAG의 leave는 입력 텐서이고, root는 결과 텐서이다.\n",
    "- 따라서 root에서 leave로 추적하면 연쇄 법칙에 따라 gradient를 자동으로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44e9aa-3533-4c43-8b82-05fe1c3fbb25",
   "metadata": {},
   "source": [
    "순전파 단계에서, autograd는 다음 두 가지 작업을 동시에 수행합니다:\n",
    "- 요청된 연산을 수행하여 결과 텐서를 계산하고,\n",
    "- DAG에 연산의 변화도 기능(gradient function) 를 유지(maintain)합니다.      \n",
    "      \n",
    "역전파 단계는 DAG 뿌리(root)에서 .backward() 가 호출될 때 시작됩니다. autograd는 이 때:\n",
    "- 각 .grad_fn 으로부터 변화도를 계산하고,\n",
    "- 각 텐서의 .grad 속성에 계산 결과를 쌓고(accumulate),\n",
    "- 연쇄 법칙을 사용하여, 모든 잎(leaf) 텐서들까지 전파(propagate)합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2fb14-4545-4bec-aaf3-06ef9837509e",
   "metadata": {},
   "source": [
    "### 선택적으로 읽기(Optional Reading): 텐서 변화도와 야코비안 곱 (Jacobian Product)\n",
    "- pytorch에서는 실제 gradient가 아닌 야코비안 곱(jacobian product)을 계산한다.\n",
    "- x에 대한 y의 변화도는 야코비안 행렬로 주어진다.\n",
    "---\n",
    "무슨 말인지 모르겠지만 일반적으로 backward로 계산하는 것보다 빠르다는 것 같음..아마도..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086c367a-aa70-4872-bc88-5c3d8bcd8a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
